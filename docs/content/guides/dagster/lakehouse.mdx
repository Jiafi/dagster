---
title: Software-Defined Assets | Dagster
description: Lakehouse is set of APIs for defining pipelines that puts assets, like database tables and ML models, at the center.
---

# Software-Defined Assets <Experimental />

<CodeReferenceLink filePath="examples/simple_software_defined_asset" />

The "Software-defined asset" APIs: sit atop of the graph/job/op APIs and enable a novel way of constructing Dagster jobs that puts assets at the forefront. As a reminder, to Dagster, an "asset" is a data product: an object produced by a data pipeline, e.g. a table, ML model, or report.

Conceptually, software-defined assets invert the typical relationship between assets and computation. Instead of defining a graph of ops and recording which assets those ops end up materializing, you define a set of assets, each of which knows how to compute its contents from upstream assets.

Taking a software-defined asset approach has a few main benefits:

* **Write less code** - because each asset knows about the assets it depends on, you don't need to use @graph / @pipeline to wire up dependencies between your ops.
* **Track cross-job dependencies via asset lineage** - Dagit allows you to find the parents and children of any asset, even if they live in different jobs. This is useful for finding the sources of problems and for understanding the consequences of changing or removing an asset.
* **Know when you need to take action on an asset** - In a unified view, Dagster compares the assets you've defined in code to the assets you've materialized in storage. You can catch that you've deployed code for generating a new table, but that you haven't yet materialized it. Or that you've deployed code that adds a column to a table, but that your stored table is still missing that column. Or that you've removed an asset definition, but the table still exists in storage.

In this example, we'll define some tables and generate a Dagster job that updates them. We have a table of temperature samples collected in five-minute increments, and we want to compute a table of the highest temperatures for each day.

## Data Assets

Here are our asset (aka table) definitions.

```python file=../../simple_lakehouse/simple_lakehouse/assets.py
"""Asset definitions for the simple_lakehouse example."""
import pandas as pd
from lakehouse import Column, computed_table, source_table
from pyarrow import date32, float64, string

sfo_q2_weather_sample_table = source_table(
    path="data",
    columns=[Column("tmpf", float64()), Column("valid_date", string())],
)


@computed_table(
    input_assets=[sfo_q2_weather_sample_table],
    columns=[Column("valid_date", date32()), Column("max_tmpf", float64())],
)
def daily_temperature_highs_table(sfo_q2_weather_sample: pd.DataFrame) -> pd.DataFrame:
    """Computes the temperature high for each day"""
    sfo_q2_weather_sample["valid_date"] = pd.to_datetime(sfo_q2_weather_sample["valid"])
    return sfo_q2_weather_sample.groupby("valid_date").max().rename(columns={"tmpf": "max_tmpf"})
```

`sfo_q2_weather_sample` represents our base temperature table.  It's a `ForeignAsset`, meaning that it's generated outside Dagster.

`daily_temperature_highs` represents a computed asset.  It's derived by taking the `sfo_q2_weather_sample` table and applying the decorated function to it.  Notice that it's defined using a pure function - i.e. a function with no side effects, just logical data transformation.  The code for storing and retrieving the data in persistent storage will be supplied later on - that allows swapping in different implementations in different environments. E.g. we might want to store data in a local csv file for easy testing, but store data a data warehouse in production.

The framework infers that `daily_temperature_highs` depends on `sfo_q2_weather_sample`, by looking at the name of the argument to the decorated function.

## Storage

```python file=../../simple_lakehouse/simple_lakehouse/lakehouse_def.py
"""
This defines a Lakehouse with local storage and Pandas data processing.

Data is locally stored in csv files.

Pandas is used for data processing.  Data can be read from CSV files into a
pandas dataframe, and exported back into pandas dataframes.
"""
import os
from typing import Tuple

import pandas as pd
from dagster import ModeDefinition, StringSource, resource
from lakehouse import AssetStorage, Lakehouse


class LocalFileSystemStorage(AssetStorage):
    def __init__(self, root):
        self._root = root

    def _get_fs_path(self, path: Tuple[str, ...]) -> str:
        rpath = os.path.join(self._root, *path) + ".csv"
        return os.path.abspath(rpath)

    def save(self, obj: pd.DataFrame, path: Tuple[str, ...], _resources) -> None:
        """This saves the dataframe as a CSV."""
        fpath = self._get_fs_path(path)
        obj.to_csv(fpath)

    def load(self, _python_type, path: Tuple[str, ...], _resources):
        """This reads a dataframe from a CSV."""
        fpath = self._get_fs_path(path)
        return pd.read_csv(fpath)


@resource(config_schema={"root": StringSource})
def local_fs_storage(init_context):
    return LocalFileSystemStorage(init_context.resource_config["root"])


simple_lakehouse = Lakehouse(
    mode_defs=[
        ModeDefinition(
            resource_defs={"default_storage": local_fs_storage.configured({"root": "."})},
        )
    ]
)
```

We want to persist the data to disk using csv files. Then, we need to create an `AssetStorage` to describe the conversion between pandas dataframes and csv files.

The `load` function converts inputs to the required format for an asset. Since our base asset will represent a csv file, and our second asset will be processing a pandas dataframe, `load` will convert a csv to a dataframe. Likewise, we want to persist the results of our second asset as a csv file, so our `save` method converts a pandas dataframe to a csv.

Then, we construct our `Lakehouse`, which delegates conversion and storage between assets by utilizing the `AssetStorage` we just defined.

## Pipeline

The data assets, combined with the storage for handling conversion between data formats, completely define a computation graph. As a result, we can use the assets and storage to construct a pipeline.

```python file=../../simple_lakehouse/simple_lakehouse/pipelines.py
"""Pipeline definitions for the simple_lakehouse example.
"""
from simple_lakehouse.assets import daily_temperature_highs_table
from simple_lakehouse.lakehouse_def import simple_lakehouse

computed_assets = [daily_temperature_highs_table]
simple_lakehouse_pipeline = simple_lakehouse.build_pipeline_definition(
    "simple_lakehouse_pipeline",
    computed_assets,
)
```

Note that the assets don't have to be provided in order. Lakehouse is able to determine asset ordering by resolving input asset dependencies.
